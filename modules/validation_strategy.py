import time
from abc import ABC, abstractmethod
import torch
import torch.nn as nn
import torch.optim as optim
from torchvision import datasets, transforms
from torch.utils.data import DataLoader

class ValidationStrategy(ABC):
    @abstractmethod
    def validate(self, architecture: dict) -> dict:
        pass

class DynamicCNN(nn.Module):
    """
    This class dynamically builds a PyTorch nn.Module directly from the 
    architecture dictionary generated by your NAS SearchStrategy.
    """
    def __init__(self, architecture: dict, num_classes=10):
        super(DynamicCNN, self).__init__()
        
        feature_layers = []
        in_channels = 1 # MNIST is grayscale
        
        # 1. Build Feature Extractor
        for layer in architecture.get("layers", []):
            l_type = layer["type"]
            if l_type == "Conv2d":
                out_c = layer["channels"]
                k = layer["kernel"]
                p = layer["padding"]
                feature_layers.append(nn.Conv2d(in_channels, out_c, k, padding=p))
                in_channels = out_c
            elif l_type == "MaxPool2d":
                feature_layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
            elif l_type == "ReLU":
                feature_layers.append(nn.ReLU())
            elif l_type == "Dropout":
                feature_layers.append(nn.Dropout(p=layer.get("rate", 0.5)))
                
        self.features = nn.Sequential(*feature_layers)
        
        # 2. Build Classifier
        classifier_layers = [nn.Flatten()]
        last_hid = architecture.get("last_hid_mlp", 0)
        
        if last_hid > 0:
            classifier_layers.append(nn.LazyLinear(last_hid))
            classifier_layers.append(nn.ReLU())
            classifier_layers.append(nn.Linear(last_hid, num_classes))
        else:
            classifier_layers.append(nn.LazyLinear(num_classes))
            
        self.classifier = nn.Sequential(*classifier_layers)

    def forward(self, x):
        x = self.features(x)
        x = self.classifier(x)
        return x


class FullTraining(ValidationStrategy):
    def __init__(self, config: dict):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"--- Initialized FullTraining Validator on Device: {self.device} ---")
        
        # Determine batch sizes from config
        train_batch = self.config.get('trn_data', {}).get('batch', 32)
        val_batch = self.config.get('val_data', {}).get('batch', 64)
        
        # Standard MNIST transformations
        transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        # Load datasets (Downloads automatically if not present)
        # Note: We use torchvision instead of custom CSV parsing for robustness and speed
        train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
        val_dataset = datasets.MNIST('./data', train=False, transform=transform)
        
        self.train_loader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True)
        self.val_loader = DataLoader(val_dataset, batch_size=val_batch, shuffle=False)

    def validate(self, architecture: dict) -> dict:
        """Builds, trains, and evaluates the dynamically generated model."""
        model = DynamicCNN(architecture).to(self.device)
        
        # Dummy pass to initialize LazyLinear weights before setting up the optimizer
        dummy_input = torch.randn(1, 1, 28, 28).to(self.device)
        _ = model(dummy_input)
        
        # Setup Optimizer based on Config
        opt_name = self.config.get('optimizer', 'Adam')
        lr = self.config.get('optimizer_params', {}).get('lr', 0.001)
        
        if opt_name == 'Adam':
            optimizer = optim.Adam(model.parameters(), lr=lr)
        else:
            optimizer = optim.SGD(model.parameters(), lr=lr)
            
        criterion = nn.CrossEntropyLoss()
        epochs = self.config.get('epochs', 5)
        
        loss_history = []
        start_time = time.time()
        
        # --- TRAINING LOOP ---
        model.train()
        for epoch in range(epochs):
            epoch_loss = 0.0
            for batch_idx, (data, target) in enumerate(self.train_loader):
                data, target = data.to(self.device), target.to(self.device)
                
                optimizer.zero_grad()
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                
            avg_loss = epoch_loss / len(self.train_loader)
            loss_history.append(avg_loss)
            print(f"  Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}")
            
        # --- VALIDATION LOOP ---
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for data, target in self.val_loader:
                data, target = data.to(self.device), target.to(self.device)
                outputs = model(data)
                _, predicted = torch.max(outputs.data, 1)
                total += target.size(0)
                correct += (predicted == target).sum().item()
                
        acc_val = correct / total
        runtime = time.time() - start_time
        
        print(f"  -> Validation Accuracy: {acc_val:.4f} (Time: {runtime:.2f}s)\n")
        
        return {
            "acc_val": acc_val,
            "loss_history": loss_history,
            "runtime": runtime
        }